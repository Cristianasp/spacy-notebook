{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contens<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#About-this-notebook\" data-toc-modified-id=\"About-this-notebook-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>About this notebook</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-to-install-spacy:\" data-toc-modified-id=\"How-to-install-spacy:-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>How to install spacy:</a></span></li><li><span><a href=\"#How-to-download-the-models:\" data-toc-modified-id=\"How-to-download-the-models:-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>How to download the models:</a></span></li><li><span><a href=\"#Using-nbextensions\" data-toc-modified-id=\"Using-nbextensions-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Using nbextensions</a></span></li></ul></li><li><span><a href=\"#Finding-words,-phrases,-names-and-concepts\" data-toc-modified-id=\"Finding-words,-phrases,-names-and-concepts-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Finding words, phrases, names and concepts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction-to-Spacy\" data-toc-modified-id=\"Introduction-to-Spacy-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Introduction to Spacy</a></span></li><li><span><a href=\"#Getting-Started\" data-toc-modified-id=\"Getting-Started-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Getting Started</a></span></li><li><span><a href=\"#Documents,-Spans-and-Tokens\" data-toc-modified-id=\"Documents,-Spans-and-Tokens-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Documents, Spans and Tokens</a></span></li><li><span><a href=\"#Lexical-Attributes\" data-toc-modified-id=\"Lexical-Attributes-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Lexical Attributes</a></span></li><li><span><a href=\"#Statistical-Models\" data-toc-modified-id=\"Statistical-Models-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Statistical Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-are-Statistical-models-?\" data-toc-modified-id=\"What-are-Statistical-models-?-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>What are Statistical models ?</a></span></li><li><span><a href=\"#Model-Packages\" data-toc-modified-id=\"Model-Packages-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Model Packages</a></span></li><li><span><a href=\"#Predicting-part-of-speech-tags\" data-toc-modified-id=\"Predicting-part-of-speech-tags-2.5.3\"><span class=\"toc-item-num\">2.5.3&nbsp;&nbsp;</span>Predicting part of speech tags</a></span></li><li><span><a href=\"#Predicting-Syntactic-Dependencies\" data-toc-modified-id=\"Predicting-Syntactic-Dependencies-2.5.4\"><span class=\"toc-item-num\">2.5.4&nbsp;&nbsp;</span>Predicting Syntactic Dependencies</a></span></li><li><span><a href=\"#Dependency-Label-Scheme\" data-toc-modified-id=\"Dependency-Label-Scheme-2.5.5\"><span class=\"toc-item-num\">2.5.5&nbsp;&nbsp;</span>Dependency Label Scheme</a></span></li><li><span><a href=\"#Predicting-Name-Entities\" data-toc-modified-id=\"Predicting-Name-Entities-2.5.6\"><span class=\"toc-item-num\">2.5.6&nbsp;&nbsp;</span>Predicting Name Entities</a></span></li><li><span><a href=\"#Tip:-the-explain-method\" data-toc-modified-id=\"Tip:-the-explain-method-2.5.7\"><span class=\"toc-item-num\">2.5.7&nbsp;&nbsp;</span>Tip: the explain method</a></span></li></ul></li><li><span><a href=\"#Model-Packages\" data-toc-modified-id=\"Model-Packages-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Model Packages</a></span></li><li><span><a href=\"#Loading-Models\" data-toc-modified-id=\"Loading-Models-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Loading Models</a></span></li><li><span><a href=\"#Predicting-Linguistic-Annotations\" data-toc-modified-id=\"Predicting-Linguistic-Annotations-2.8\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>Predicting Linguistic Annotations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Part-1\" data-toc-modified-id=\"Part-1-2.8.1\"><span class=\"toc-item-num\">2.8.1&nbsp;&nbsp;</span>Part 1</a></span></li><li><span><a href=\"#Part-2\" data-toc-modified-id=\"Part-2-2.8.2\"><span class=\"toc-item-num\">2.8.2&nbsp;&nbsp;</span>Part 2</a></span></li></ul></li><li><span><a href=\"#Predicting-name-entities-in-context\" data-toc-modified-id=\"Predicting-name-entities-in-context-2.9\"><span class=\"toc-item-num\">2.9&nbsp;&nbsp;</span>Predicting name entities in context</a></span></li><li><span><a href=\"#Rule-based-matching\" data-toc-modified-id=\"Rule-based-matching-2.10\"><span class=\"toc-item-num\">2.10&nbsp;&nbsp;</span>Rule based matching</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-the-matcher\" data-toc-modified-id=\"Using-the-matcher-2.10.1\"><span class=\"toc-item-num\">2.10.1&nbsp;&nbsp;</span>Using the matcher</a></span></li><li><span><a href=\"#Matching-lexical-attributes\" data-toc-modified-id=\"Matching-lexical-attributes-2.10.2\"><span class=\"toc-item-num\">2.10.2&nbsp;&nbsp;</span>Matching lexical attributes</a></span></li><li><span><a href=\"#Using-Operations-and-quantifiers\" data-toc-modified-id=\"Using-Operations-and-quantifiers-2.10.3\"><span class=\"toc-item-num\">2.10.3&nbsp;&nbsp;</span>Using Operations and quantifiers</a></span></li></ul></li><li><span><a href=\"#Using-the-matcher\" data-toc-modified-id=\"Using-the-matcher-2.11\"><span class=\"toc-item-num\">2.11&nbsp;&nbsp;</span>Using the matcher</a></span></li><li><span><a href=\"#Writing-Match-Patterns\" data-toc-modified-id=\"Writing-Match-Patterns-2.12\"><span class=\"toc-item-num\">2.12&nbsp;&nbsp;</span>Writing Match Patterns</a></span><ul class=\"toc-item\"><li><span><a href=\"#Part-1\" data-toc-modified-id=\"Part-1-2.12.1\"><span class=\"toc-item-num\">2.12.1&nbsp;&nbsp;</span>Part 1</a></span></li><li><span><a href=\"#Part-2\" data-toc-modified-id=\"Part-2-2.12.2\"><span class=\"toc-item-num\">2.12.2&nbsp;&nbsp;</span>Part 2</a></span></li></ul></li></ul></li><li><span><a href=\"#Large-scale-data-analysis-with-spaCy\" data-toc-modified-id=\"Large-scale-data-analysis-with-spaCy-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Large-scale data analysis with spaCy</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Structures-(1):-Vocab,-Lexemes-and-StringStore\" data-toc-modified-id=\"Data-Structures-(1):-Vocab,-Lexemes-and-StringStore-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Data Structures (1): Vocab, Lexemes and StringStore</a></span><ul class=\"toc-item\"><li><span><a href=\"#Shared-Vocab-and-string-store\" data-toc-modified-id=\"Shared-Vocab-and-string-store-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Shared Vocab and string store</a></span></li><li><span><a href=\"#Lexemes:-entries-in-the-vocabulary\" data-toc-modified-id=\"Lexemes:-entries-in-the-vocabulary-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Lexemes: entries in the vocabulary</a></span></li></ul></li><li><span><a href=\"#String-to-hashes\" data-toc-modified-id=\"String-to-hashes-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>String to hashes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Part-1\" data-toc-modified-id=\"Part-1-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Part 1</a></span></li><li><span><a href=\"#Part-2\" data-toc-modified-id=\"Part-2-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Part 2</a></span></li></ul></li><li><span><a href=\"#Vocab,-hashes-and-lexemes\" data-toc-modified-id=\"Vocab,-hashes-and-lexemes-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Vocab, hashes and lexemes</a></span></li><li><span><a href=\"#Data-Structures(2)\" data-toc-modified-id=\"Data-Structures(2)-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Data Structures(2)</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-Doc-object\" data-toc-modified-id=\"The-Doc-object-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>The Doc object</a></span></li></ul></li><li><span><a href=\"#Creating-a-Doc\" data-toc-modified-id=\"Creating-a-Doc-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Creating a Doc</a></span><ul class=\"toc-item\"><li><span><a href=\"#Part-1\" data-toc-modified-id=\"Part-1-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Part 1</a></span></li><li><span><a href=\"#Part-2\" data-toc-modified-id=\"Part-2-3.5.2\"><span class=\"toc-item-num\">3.5.2&nbsp;&nbsp;</span>Part 2</a></span></li><li><span><a href=\"#Part-3\" data-toc-modified-id=\"Part-3-3.5.3\"><span class=\"toc-item-num\">3.5.3&nbsp;&nbsp;</span>Part 3</a></span></li></ul></li><li><span><a href=\"#Docs,-spans-and-entities-from-scratch\" data-toc-modified-id=\"Docs,-spans-and-entities-from-scratch-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Docs, spans and entities from scratch</a></span></li><li><span><a href=\"#Data-structures-and-best-practices\" data-toc-modified-id=\"Data-structures-and-best-practices-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Data structures and best practices</a></span><ul class=\"toc-item\"><li><span><a href=\"#Part-1\" data-toc-modified-id=\"Part-1-3.7.1\"><span class=\"toc-item-num\">3.7.1&nbsp;&nbsp;</span>Part 1</a></span></li><li><span><a href=\"#Part-2\" data-toc-modified-id=\"Part-2-3.7.2\"><span class=\"toc-item-num\">3.7.2&nbsp;&nbsp;</span>Part 2</a></span></li></ul></li><li><span><a href=\"#Word-vectors-and-semantic-similarities\" data-toc-modified-id=\"Word-vectors-and-semantic-similarities-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Word vectors and semantic similarities</a></span><ul class=\"toc-item\"><li><span><a href=\"#Similarity-examples\" data-toc-modified-id=\"Similarity-examples-3.8.1\"><span class=\"toc-item-num\">3.8.1&nbsp;&nbsp;</span>Similarity examples</a></span></li></ul></li><li><span><a href=\"#Inspecting-word-vectors\" data-toc-modified-id=\"Inspecting-word-vectors-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>Inspecting word vectors</a></span></li><li><span><a href=\"#Comparing-similarities\" data-toc-modified-id=\"Comparing-similarities-3.10\"><span class=\"toc-item-num\">3.10&nbsp;&nbsp;</span>Comparing similarities</a></span></li><li><span><a href=\"#Combining-models-and-rules\" data-toc-modified-id=\"Combining-models-and-rules-3.11\"><span class=\"toc-item-num\">3.11&nbsp;&nbsp;</span>Combining models and rules</a></span><ul class=\"toc-item\"><li><span><a href=\"#Statistical-predictions-vs.-rules\" data-toc-modified-id=\"Statistical-predictions-vs.-rules-3.11.1\"><span class=\"toc-item-num\">3.11.1&nbsp;&nbsp;</span>Statistical predictions vs. rules</a></span></li><li><span><a href=\"#RECAP:-Rule-based-matching\" data-toc-modified-id=\"RECAP:-Rule-based-matching-3.11.2\"><span class=\"toc-item-num\">3.11.2&nbsp;&nbsp;</span>RECAP: Rule-based matching</a></span></li><li><span><a href=\"#Adding-statistical-predictions\" data-toc-modified-id=\"Adding-statistical-predictions-3.11.3\"><span class=\"toc-item-num\">3.11.3&nbsp;&nbsp;</span>Adding statistical predictions</a></span></li><li><span><a href=\"#Efficient-phrase-matching-(1)\" data-toc-modified-id=\"Efficient-phrase-matching-(1)-3.11.4\"><span class=\"toc-item-num\">3.11.4&nbsp;&nbsp;</span>Efficient phrase matching (1)</a></span></li></ul></li><li><span><a href=\"#Debugging-patterns-(1)\" data-toc-modified-id=\"Debugging-patterns-(1)-3.12\"><span class=\"toc-item-num\">3.12&nbsp;&nbsp;</span>Debugging patterns (1)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Debuggind-patterns-(2)\" data-toc-modified-id=\"Debuggind-patterns-(2)-3.12.1\"><span class=\"toc-item-num\">3.12.1&nbsp;&nbsp;</span>Debuggind patterns (2)</a></span></li><li><span><a href=\"#Efficient-phrase-matching-------\" data-toc-modified-id=\"Efficient-phrase-matching--------3.12.2\"><span class=\"toc-item-num\">3.12.2&nbsp;&nbsp;</span>Efficient phrase matching ------</a></span></li><li><span><a href=\"#Extracting-countries-and-relationships-----\" data-toc-modified-id=\"Extracting-countries-and-relationships------3.12.3\"><span class=\"toc-item-num\">3.12.3&nbsp;&nbsp;</span>Extracting countries and relationships ----</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this notebook\n",
    "\n",
    "This notebook is based on Ines Montani's free online course, available at:\n",
    "\n",
    "https://course.spacy.io/\n",
    "\n",
    "https://github.com/ines/spacy-course\n",
    "\n",
    "\n",
    "## How to install spacy:\n",
    "\n",
    "The instructions are in the link below :\n",
    "\n",
    "https://spacy.io/usage\n",
    "\n",
    "To install spacy, please use:\n",
    "\n",
    "- conda install -c conda-forge spacy\n",
    "\n",
    "This notebook is tested for version 2.1.3\n",
    "\n",
    "- As per May, 4th 2019, installing Spacy via \"conda install -c conda-forge spacy\" delivers 2.0.8 version so I used \"pip install -U spacy\" to have version 2.1.3 in my computer\n",
    "\n",
    "## How to download the models:\n",
    "\n",
    "https://github.com/explosion/spacy-models/releases/\n",
    "\n",
    "To download a model:\n",
    "\n",
    "- python -m spacy download pt\n",
    "- python -m spacy download en\n",
    "\n",
    "To download a model:\n",
    "- python -m spacy download en_core_web_sm\n",
    "- python -m spacy download pt_core_news_sm\n",
    "- python -m spacy download en_core_web_md\n",
    "- python -m spacy download pt_core_news_md - there is not... :-(\n",
    "\n",
    "## Using nbextensions\n",
    "\n",
    "I highly recommend you to install table of contents from nbextensions, that makes the navigation in the sections much more easier.\n",
    "\n",
    "Instructions can be found here:\n",
    "\n",
    "https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/install.html\n",
    "\n",
    "- conda install -c conda-forge jupyter_contrib_nbextensions\n",
    "\n",
    "- jupyter nbextension enable toc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.3\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding words, phrases, names and concepts\n",
    "\n",
    "https://course.spacy.io/chapter1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Spacy\n",
    "\n",
    "At the center of spaCy is the object containing the processing pipeline. We usually call this variable \"nlp\".\n",
    "\n",
    "For example, to create an English nlp object, you can import the English language class from spacy dot lang dot en and instantiate it. You can use the nlp object like a function to analyze text.\n",
    "\n",
    "It contains all the different components in the pipeline.\n",
    "\n",
    "It also includes language-specific rules used for tokenizing the text into words and punctuation. spaCy supports a variety of languages that are available in spacy dot lang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the English language class\n",
    "from spacy.lang.pt import Portuguese\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = Portuguese()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.pt.Portuguese at 0x1f18c2f25f8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you process a text with the nlp object, spaCy creates a Doc object – short for \"document\". The Doc lets you access information about the text in a structured way, and no information is lost.\n",
    "\n",
    "The Doc behaves like a normal Python sequence by the way and lets you iterate over its tokens, or get a token by its index. But more on that later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olá\n",
      "mundo\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# Created by processing a string of text with the nlp object\n",
    "doc = nlp(\"Olá mundo!\")\n",
    "\n",
    "# Iterate over tokens in a Doc\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/token.png' ></img> \n",
    "\n",
    "Token objects represent the tokens in a document – for example, a word or a punctuation character.\n",
    "\n",
    "To get a token at a specific position, you can index into the Doc.\n",
    "\n",
    "Token objects also provide various attributes that let you access more information about the tokens. For example, the dot text attribute returns the verbatim token text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mundo\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Oi mundo!\")\n",
    "\n",
    "# Index into the Doc to get a single Token\n",
    "token = doc[1]\n",
    "\n",
    "# Get the token text via the .text attribute\n",
    "print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A Span object is a slice of the document consisting of one or more tokens. It's only a view of the Doc and doesn't contain any data itself.\n",
    "\n",
    "To create a Span, you can use Python's slice notation. For example, 1 colon 3 will create a slice starting from the token at position 1, up to – but not including! – the token at position 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mundo!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Oi mundo!\")\n",
    "\n",
    "# A slice from the Doc is a Span object\n",
    "span = doc[1:4]\n",
    "\n",
    "# Get the span text via the .text attribute\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see some of the available token attributes:\n",
    "\n",
    "\"i\" is the index of the token within the parent document.\n",
    "\n",
    "\"text\" returns the token text.\n",
    "\n",
    "\"is alpha\", \"is punct\" and \"like num\" return boolean values indicating whether the token consists of alphanumeric characters, whether it's punctuation or whether it resembles a number. For example, a token \"10\" – one, zero – or the word \"ten\" – T, E, N.\n",
    "\n",
    "These attributes are also called lexical attributes: they refer to the entry in the vocabulary and don't depend on the token's context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:    [0, 1, 2, 3, 4, 5]\n",
      "Text:     ['O', 'ingresso', 'custa', '$', '50', '.']\n",
      "is_alpha: [True, True, True, False, False, False]\n",
      "is_punct: [False, False, False, False, False, True]\n",
      "like_num: [False, False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"O ingresso custa $50.\")\n",
    "\n",
    "print('Index:   ', [token.i for token in doc])\n",
    "print('Text:    ', [token.text for token in doc])\n",
    "\n",
    "print('is_alpha:', [token.is_alpha for token in doc])\n",
    "print('is_punct:', [token.is_punct for token in doc])\n",
    "print('like_num:', [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Let’s get started and try out spaCy! In this exercise, you’ll be able to try out some of the 45+ available languages.\n",
    "\n",
    "Part 1: English\n",
    "\n",
    "Import the English class from spacy.lang.en and create the nlp object.\n",
    "\n",
    "Create a doc and print its text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essa é uma frase.\n"
     ]
    }
   ],
   "source": [
    "# Import the English language class\n",
    "from spacy.lang.pt import Portuguese\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = Portuguese()\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Essa é uma frase.\")\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents, Spans and Tokens\n",
    "\n",
    "When you call nlp on a string, spaCy first tokenizes the text and creates a document object. In this exercise, you’ll learn more about the Doc, as well as its views Token and Span.\n",
    "\n",
    "Step 1\n",
    "\n",
    "Import the English language class and create the nlp object.\n",
    "\n",
    "Process the text and instantiate a Doc object in the variable doc.\n",
    "\n",
    "Select the first token of the Doc and print its text.\n",
    "\n",
    "Create a slice of the Doc for the tokens “tree kangaroos” and “tree kangaroos and narwhals”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n"
     ]
    }
   ],
   "source": [
    "# Import the English language class and create the nlp object\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\"I like tree kangaroos and narwhals.\")\n",
    "\n",
    "# Select the first token\n",
    "first_token = doc[0]\n",
    "\n",
    "# Print the first token's text\n",
    "print(first_token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the English language class and create the nlp object.\n",
    "\n",
    "Process the text and instantiate a Doc object in the variable doc.\n",
    "\n",
    "Create a slice of the Doc for the tokens “tree kangaroos” and “tree kangaroos and narwhals”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree kangaroos\n",
      "tree kangaroos and narwhals\n"
     ]
    }
   ],
   "source": [
    "# Import the English language class and create the nlp object\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\"I like tree kangaroos and narwhals.\")\n",
    "\n",
    "# A slice of the Doc for \"tree kangaroos\"\n",
    "tree_kangaroos = doc[2:4]\n",
    "print(tree_kangaroos.text)\n",
    "\n",
    "# A slice of the Doc for \"tree kangaroos and narwhals\" (without the \".\")\n",
    "tree_kangaroos_and_narwhals = doc[2:6]\n",
    "print(tree_kangaroos_and_narwhals.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Attributes\n",
    "\n",
    "In this example, you’ll use spaCy’s Doc and Token objects, and lexical attributes to find percentages in a text. You’ll be looking for two subsequent tokens: a number and a percent sign.\n",
    "\n",
    "Use the like_num token attribute to check whether a token in the doc resembles a number.\n",
    "\n",
    "Get the token following the current token in the document. The index of the next token in the doc is token.i + 1.\n",
    "\n",
    "Check whether the next token’s text attribute is a percent sign ”%“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 60\n",
      "Percentage found: 4\n",
      "like_num: [False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
    "    \"Now less than 4% are.\"\n",
    ")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i + 1]\n",
    "        # Check if the next token's text equals '%'\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.text)\n",
    "            \n",
    "            \n",
    "print('like_num:', [token.like_num for token in doc])            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Models\n",
    "\n",
    "Some of the most interesting things you can analyze are context-specific: for example, whether a word is a verb or whether a span of text is a person name.\n",
    "\n",
    "### What are Statistical models ?\n",
    "\n",
    "Statistical models enable spaCy to make predictions in context. This usually includes part-of speech tags, syntactic dependencies and named entities.\n",
    "\n",
    "Models are trained on large datasets of labeled example texts.\n",
    "\n",
    "They can be updated with more examples to fine-tune their predictions – for example, to perform better on your specific data.\n",
    "\n",
    "### Model Packages\n",
    "\n",
    "spaCy provides a number of pre-trained model packages you can download using the \"spacy download\" command. For example, the \"en_core_web_sm\" package is a small English model that supports all core capabilities and is trained on web text.\n",
    "\n",
    "The spacy dot load method loads a model package by name and returns an nlp object.\n",
    "\n",
    "The package provides the binary weights that enable spaCy to make predictions.\n",
    "\n",
    "It also includes the vocabulary, and meta information to tell spaCy which language class to use and how to configure the processing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/models.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit:\n",
    "https://github.com/explosion/spacy-models/releases/\n",
    "    \n",
    "Remember, to download: \n",
    "\n",
    " python -m spacy download pt_core_news_sm\n",
    " \n",
    " python -m spacy download en_core_news_sm\n",
    " \n",
    " python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ela PRON\n",
      "comeu VERB\n",
      "uma DET\n",
      "pizza NOUN\n",
      "enorme ADJ\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small Portuguese model\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Ela comeu uma pizza enorme\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting part of speech tags\n",
    "\n",
    "For each token in the Doc, we can print the text and the \"pos underscore\" attribute, the predicted part-of-speech tag.\n",
    "\n",
    "In spaCy, attributes that return strings usually end with an underscore – attributes without the underscore return an ID.\n",
    "\n",
    "Here, the model correctly predicted \"ate\" as a verb and \"pizza\" as a noun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the model's predictions. In this example, we're using spaCy to predict part-of-speech tags, the word types in context.\n",
    "\n",
    "First, we load the small English model and receive an nlp object.\n",
    "\n",
    "Next, we're processing the text \"She ate the pizza\".\n",
    "\n",
    "For each token in the Doc, we can print the text and the \"pos underscore\" attribute, the predicted part-of-speech tag.\n",
    "\n",
    "In spaCy, attributes that return strings usually end with an underscore – attributes without the underscore return an ID.\n",
    "\n",
    "Here, the model correctly predicted \"ate\" as a verb and \"pizza\" as a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp= spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc= nlp(\"She ate the pizza\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ela PRON\n",
      "comeu VERB\n",
      "o DET\n",
      "macarrão NOUN\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Ela comeu o macarrão\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ela PRON\n",
      "comeu VERB\n",
      "o DET\n",
      "macarrão NOUN\n",
      "com ADP\n",
      "molho NOUN\n",
      "de ADP\n",
      "tomate NOUN\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"Ela comeu o macarrão com molho de tomate\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quero VERB\n",
      "comprar VERB\n",
      "um DET\n",
      "ingresso NOUN\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"Quero comprar um ingresso\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Syntactic Dependencies\n",
    "\n",
    "In addition to the part-of-speech tags, we can also predict how the words are related. For example, whether a word is the subject of the sentence or an object.\n",
    "\n",
    "The \"dep underscore\" attribute returns the predicted dependency label.\n",
    "\n",
    "The head attribute returns the syntactic head token. You can also think of it as the parent token this word is attached to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quero VERB ROOT Quero\n",
      "comprar VERB xcomp Quero\n",
      "um DET det ingresso\n",
      "ingresso NOUN obj comprar\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Label Scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/dep_example.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To describe syntactic dependencies, spaCy uses a standardized label scheme. Here's an example of some common labels:\n",
    "\n",
    "The pronoun \"She\" is a nominal subject attached to the verb – in this case, to \"ate\".\n",
    "\n",
    "The noun \"pizza\" is a direct object attached to the verb \"ate\". It is eaten by the subject, \"she\".\n",
    "\n",
    "The determiner \"the\", also known as an article, is attached to the noun \"pizza\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Name Entities\n",
    "\n",
    "Named entities are \"real world objects\" that are assigned a name – for example, a person, an organization or a country.\n",
    "\n",
    "The doc dot ents property lets you access the named entities predicted by the model.\n",
    "\n",
    "It returns an iterator of Span objects, so we can print the entity text and the entity label using the \"label underscore\" attribute.\n",
    "\n",
    "In this case, the model is correctly predicting \"Apple\" as an organization, \"U.K.\" as a geopolitical entity and \"$1 billion\" as money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp= spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc= nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "Inglaterra LOC\n",
      "R$ PER\n"
     ]
    }
   ],
   "source": [
    "# Load the small Portuguese model\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(u\"A empresa Apple está buscando comprar uma empresa na Inglaterra por R$1 bilhão de reais\")\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip: the explain method\n",
    "\n",
    "A quick tip: To get definitions for the most common tags and labels, you can use the spacy dot explain helper function.\n",
    "\n",
    "For example, \"GPE\" for geopolitical entity isn't exactly intuitive – but spacy dot explain can tell you that it refers to countries, cities and states.\n",
    "\n",
    "The same works for part-of-speech tags and dependency labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Named person or family.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('PER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noun, proper singular'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('NNP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'direct object'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('dobj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Miscellaneous entities, e.g. events, nationalities, products or works of art'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Non-GPE locations, mountain ranges, bodies of water'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('LOC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Loading Models\n",
    "\n",
    "The models we’re using in this course are already pre-installed. For more details on spaCy’s statistical models and how to install them on your machine, see the documentation.\n",
    "\n",
    "https://spacy.io/usage/models\n",
    "\n",
    "Use spacy.load to load the small English model 'en_core_web_sm'.\n",
    "Process the text and print the document text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the 'en_core_web_sm' model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Linguistic Annotations\n",
    "\n",
    "You’ll now get to try one of spaCy’s pre-trained model packages and see its predictions in action. Feel free to try it out on your own text! To find out what a tag or label means, you can call spacy.explain in the. For example: spacy.explain('PROPN') or spacy.explain('GPE').\n",
    "\n",
    "### Part 1\n",
    "\n",
    "Process the text with the nlp object and create a doc.\n",
    "\n",
    "For each token, print the token text, the token’s .pos_ (part-of-speech tag) and the token’s .dep_ (dependency label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It          PRON      nsubj     \n",
      "’s          PROPN     ROOT      \n",
      "official    NOUN      acomp     \n",
      ":           PUNCT     punct     \n",
      "Apple       PROPN     nsubj     \n",
      "is          VERB      ROOT      \n",
      "the         DET       det       \n",
      "first       ADJ       amod      \n",
      "U.S.        PROPN     nmod      \n",
      "public      ADJ       amod      \n",
      "company     NOUN      attr      \n",
      "to          PART      aux       \n",
      "reach       VERB      relcl     \n",
      "a           DET       det       \n",
      "$           SYM       quantmod  \n",
      "1           NUM       compound  \n",
      "trillion    NUM       nummod    \n",
      "market      NOUN      compound  \n",
      "value       NOUN      dobj      \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # This is for formatting only\n",
    "    print(\"{:<12}{:<10}{:<10}\".format(token_text, token_pos, token_dep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'direct object'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('dobj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'attribute'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('attr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nominal subject'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('nsubj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "É           VERB      cop       \n",
      "oficial     ADJ       ROOT      \n",
      ":           PUNCT     punct     \n",
      "A           DET       det       \n",
      "Apple       PROPN     nsubj     \n",
      "é           VERB      cop       \n",
      "a           DET       det       \n",
      "primeira    ADJ       amod      \n",
      "empresa     NOUN      ROOT      \n",
      "americana   ADJ       amod      \n",
      "a           ADP       mark      \n",
      "atingir     VERB      acl       \n",
      "o           DET       det       \n",
      "valor       NOUN      obj       \n",
      "de          ADP       case      \n",
      "mercado     NOUN      nmod      \n",
      "de          ADP       case      \n",
      "1           NUM       nummod    \n",
      "bilhão      NOUN      nmod      \n",
      "de          ADP       case      \n",
      "reais       NOUN      nmod      \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "text = \"É oficial: A Apple é a primeira empresa americana a atingir o valor de mercado de 1 bilhão de reais\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # This is for formatting only\n",
    "    print(\"{:<12}{:<10}{:<10}\".format(token_text, token_pos, token_dep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'copula'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('cop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('ROOT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('acl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('nummod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'modifier of nominal'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('nmod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Process the text and create a doc object.\n",
    "\n",
    "Iterate over the doc.ents and print the entity text and label_ attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "first ORDINAL\n",
      "U.S. GPE\n",
      "$1 trillion MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "text = \"É oficial: A Apple é a primeira empresa americana a atingir o valor de mercado de 1 bilhão de reais\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "text = \"Os carros de passeio estão custando 120% mais caros\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inglaterra LOC\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "text = \"O rei da Inglaterra morreu de acidente de carro\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "England GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"The king of England died on a car accident\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 dollars MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"I want to buy a tennis shoes and pay 120 dollars\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting name entities in context\n",
    "\n",
    "Models are statistical and not always right. Whether their predictions are correct depends on the training data and the text you’re processing. Let’s take a look at an example.\n",
    "\n",
    "Process the text with the nlp object.\n",
    "\n",
    "Iterate over the entities and print the entity text and label.\n",
    "\n",
    "Looks like the model didn’t predict “iPhone X”. Create a span for those tokens manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "Missing entity: iPhone X\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"New iPhone X release date leaked as Apple reveals pre-orders by mistake\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the entities\n",
    "for token in doc.ents:\n",
    "    # Print the entity text and label\n",
    "    print(token.text, token.label_)\n",
    "\n",
    "# Get the span for \"iPhone X\"\n",
    "iphone_x = doc[1:3]\n",
    "\n",
    "# Print the span text\n",
    "print(\"Missing entity:\", iphone_x.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule based matching\n",
    "\n",
    "Now we'll take a look at spaCy's matcher, which lets you write rules to find words and phrases in text.\n",
    "\n",
    "Compared to regular expressions, the matcher works with Doc and Token objects instead of only strings.\n",
    "\n",
    "It's also more flexible: you can search for texts but also other lexical attributes.\n",
    "\n",
    "You can even write rules that use the model's predictions.\n",
    "\n",
    "For example, find the word \"duck\" only if it's a verb, not a noun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not just regular expressions ?\n",
    "\n",
    "<ul>\n",
    "<li>Match on Doc objects, not just strings</li>\n",
    "\n",
    "<li>Match on tokens and token attributes</li>\n",
    "\n",
    "<li>Use the model's predictions</li>\n",
    "\n",
    "<li>Example: \"duck\" (verb) vs. \"duck\" (noun)</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match patterns are lists of dictionaries. Each dictionary describes one token. The keys are the names of token attributes, mapped to their expected values.\n",
    "\n",
    "In this example, we're looking for two tokens with the text \"iPhone\" and \"X\".\n",
    "\n",
    "We can also match on other token attributes. Here, we're looking for two tokens whose lowercase forms equal \"iphone\" and \"x\".\n",
    "\n",
    "We can even write patterns using attributes predicted by the model. Here, we're matching a token with the lemma \"buy\", plus a noun. The lemma is the base form, so this pattern would match phrases like \"buying milk\" or \"bought flowers\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Match patterns</b>\n",
    "\n",
    "<ul>\n",
    "<li>Lists of dictionaries, one per token</li>\n",
    "\n",
    "<li>Match exact token texts \n",
    "\n",
    "[{'ORTH': 'iPhone'}, {'ORTH': 'X'}]\n",
    "\n",
    "<li>Match lexical attributes\n",
    "    \n",
    "[{'LOWER': 'iphone'}, {'LOWER': 'x'}] </li>\n",
    "\n",
    "<li> Match any token attributes\n",
    "\n",
    "[{'LEMMA': 'buy'}, {'POS': 'NOUN'}]\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the matcher\n",
    "\n",
    "To use a pattern, we first import the matcher from spacy dot matcher.\n",
    "\n",
    "We also load a model and create the nlp object.\n",
    "\n",
    "The matcher is initialized with the shared vocabulary, nlp dot vocab. You'll learn more about this later – for now, just remember to always pass it in.\n",
    "\n",
    "The matcher dot add method lets you add a pattern. The first argument is a unique ID to identify which pattern was matched. The second argument is an optional callback. We don't need one here, so we set it to None. The third argument is the pattern.\n",
    "\n",
    "To match the pattern on a text, we can call the matcher on any doc.\n",
    "\n",
    "This will return the matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a model and create the nlp object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "pattern = [{'ORTH': 'iPhone'}, {'ORTH': 'X'}]\n",
    "matcher.add('IPHONE_PATTERN', None, pattern)\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(\"New iPhone X release date leaked\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9528407286733565721, 1, 3)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call the matcher on a doc, it returns a list of tuples.\n",
    "\n",
    "Each tuple consists of three values: the match ID, the start index and the end index of the matched span.\n",
    "\n",
    "This means we can iterate over the matches and create a Span object: a slice of the doc at the start and end index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "# Call the matcher on the doc\n",
    "doc = nlp(\"New iPhone X release date leaked\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "match_id: hash value of the pattern name\n",
    "\n",
    "start: start index of matched span\n",
    "\n",
    "end: end index of matched span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching lexical attributes\n",
    "\n",
    "Here's an example of a more complex pattern using lexical attributes.\n",
    "\n",
    "We're looking for five tokens:\n",
    "\n",
    "A token consisting of only digits.\n",
    "\n",
    "Three case-insensitive tokens for \"fifa\", \"world\" and \"cup\".\n",
    "\n",
    "And a token that consists of punctuation.\n",
    "\n",
    "The pattern matches the tokens \"2018 FIFA World Cup:\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 FIFA World Cup:\n"
     ]
    }
   ],
   "source": [
    "pattern = [\n",
    "    {'IS_DIGIT': True},\n",
    "    {'LOWER': 'fifa'},\n",
    "    {'LOWER': 'world'},\n",
    "    {'LOWER': 'cup'},\n",
    "    {'IS_PUNCT': True}\n",
    "]\n",
    "\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add('FIFA', None, pattern)\n",
    "\n",
    "# Process some text\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we're looking for two tokens:\n",
    "\n",
    "A verb with the lemma \"love\", followed by a noun.\n",
    "\n",
    "This pattern will match \"loved dogs\" and \"love cats\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved dogs\n",
      "love cats\n"
     ]
    }
   ],
   "source": [
    "pattern = [\n",
    "    {'LEMMA': 'love', 'POS': 'VERB'},\n",
    "    {'POS': 'NOUN'}\n",
    "]\n",
    "\n",
    "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add('PETS', None, pattern)\n",
    "\n",
    "# Process some text\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved dogs\n",
      "love cats\n"
     ]
    }
   ],
   "source": [
    "pattern = [\n",
    "    {'LEMMA': 'love', 'POS': 'VERB'},\n",
    "    {'POS': 'NOUN'}\n",
    "]\n",
    "\n",
    "doc = nlp(\"I loved dogs. Now I love cats more.\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add('PETS', None, pattern)\n",
    "\n",
    "# Process some text\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Operations and quantifiers\n",
    "\n",
    "Operators and quantifiers let you define how often a token should be matched. They can be added using the \"OP\" key.\n",
    "\n",
    "Here, the \"?\" operator makes the determiner token optional, so it will match a token with the lemma \"buy\", an optional article and a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bought a smartphone\n",
      "buying apps\n"
     ]
    }
   ],
   "source": [
    "pattern = [\n",
    "    {'LEMMA': 'buy'},\n",
    "    {'POS': 'DET', 'OP': '?'},  # optional: match 0 or 1 times\n",
    "    {'POS': 'NOUN'}\n",
    "]\n",
    "\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add('CONSUME', None, pattern)\n",
    "\n",
    "# Process some text\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"OP\" can have one of four values:\n",
    "\n",
    "An \"!\" negates the token, so it's matched 0 times.\n",
    "\n",
    "A \"?\" makes the token optional, and matches it 0 or 1 times.\n",
    "\n",
    "A \"+\" matches a token 1 or more times.\n",
    "\n",
    "And finally, an \"*\" matches 0 or more times.\n",
    "\n",
    "Operators can make your patterns a lot more powerful, but they also add more complexity – so use them wisely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'OP': '!'}\tNegation: match 0 times\n",
    "\n",
    "{'OP': '?'}\tOptional: match 0 or 1 times\n",
    "\n",
    "{'OP': '+'}\tMatch 1 or more times\n",
    "\n",
    "{'OP': '*'}\tMatch 0 or more times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the matcher \n",
    "\n",
    "Let’s try spaCy’s rule-based Matcher. You’ll be using the example from the previous exercise and write a pattern that can match the phrase “iPhone X” in the text.\n",
    "\n",
    "Import the Matcher from spacy.matcher.\n",
    "\n",
    "Initialize it with the nlp object’s shared vocab.\n",
    "\n",
    "Create a pattern that matches the 'TEXT' values of two tokens: \"iPhone\" and \"X\".\n",
    "    \n",
    "Use the matcher.add method to add the pattern to the matcher.\n",
    "\n",
    "Call the matcher on the doc and store the result in the variable matches.\n",
    "\n",
    "Iterate over the matches and get the matched span from the start to the end index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New iPhone X release date leaked as Apple reveals pre-orders by mistake\n",
      "Matches: ['iPhone X']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"New iPhone X release date leaked as Apple reveals pre-orders by mistake\")\n",
    "print(doc.text)\n",
    "\n",
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Initialize the Matcher with the shared vocabulary\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Create a pattern matching two tokens: \"iPhone\" and \"X\"\n",
    "pattern = [{'ORTH': 'iPhone'}, {'ORTH': 'X'}]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add('IPHONE_PATTERN', None, pattern)\n",
    "\n",
    "# Use the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Match Patterns\n",
    "\n",
    "In this exercise, you’ll practice writing more complex match patterns using different token attributes and operators.\n",
    "\n",
    "### Part 1\n",
    "\n",
    "Write one pattern that only matches mentions of the full iOS versions: “iOS 7”, “iOS 11” and “iOS 10”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 3\n",
      "Match found: iOS 7\n",
      "Match found: iOS 11\n",
      "Match found: iOS 10\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"After making the iOS update you won't notice a radical system-wide \"\n",
    "    \"redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of \"\n",
    "    \"iOS 11's furniture remains the same as in iOS 10. But you will discover \"\n",
    "    \"some tweaks once you delve a little deeper.\"\n",
    ")\n",
    "\n",
    "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
    "pattern = [{\"TEXT\": 'iOS'}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"IOS_VERSION_PATTERN\", None, pattern)\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Write one pattern that only matches forms of “download” (tokens with the lemma “download”), followed by a token with the part-of-speech tag 'PROPN' (proper noun)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 3\n",
      "Match found: downloaded Fortnite\n",
      "Match found: downloading Minecraft\n",
      "Match found: download Winzip\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"i downloaded Fortnite on my laptop and can't open the game at all. Help? \"\n",
    "    \"so when I was downloading Minecraft, I got the Windows version where it \"\n",
    "    \"is the '.zip' folder and I used the default program to unpack it... do \"\n",
    "    \"I also need to download Winzip?\"\n",
    ")\n",
    "\n",
    "# Write a pattern that matches a form of \"download\" plus proper noun\n",
    "pattern = [{\"LEMMA\": 'download'}, {\"POS\": 'PROPN'}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"DOWNLOAD_THINGS_PATTERN\", None, pattern)\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"Eu baixei varios aplicativos na minha vida \"\n",
    "    \"mas quando eu baixo o fortinite sempre dá problemas \"\n",
    "    \"eu realmente nao sei mais o que fazer \"\n",
    "    \"Tem algum problema para baixar o winzip?\"\n",
    ")\n",
    "\n",
    "# Write a pattern that matches a form of \"download\" plus proper noun\n",
    "pattern = [{\"LEMMA\": 'baixar'}, {\"POS\": 'PROPN'}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"DOWNLOAD_THINGS_PATTERN\", None, pattern)\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3\n",
    "\n",
    "Write one pattern that matches adjectives ('ADJ') followed by one or two 'NOUN's (one noun and one optional noun)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 4\n",
      "Match found: beautiful design\n",
      "Match found: smart search\n",
      "Match found: automatic labels\n",
      "Match found: optional voice responses\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"Features of the app include a beautiful design, smart search, automatic \"\n",
    "    \"labels and optional voice responses.\"\n",
    ")\n",
    "\n",
    "# Write a pattern for adjective plus one or two nouns\n",
    "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\", \"OP\": \"?\"}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"ADJ_NOUN_PATTERN\", None, pattern)\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-scale data analysis with spaCy\n",
    "\n",
    "https://course.spacy.io/chapter2\n",
    "\n",
    "In this chapter, you'll use your new skills to extract specific information from large volumes of text. You''ll learn how to make the most of spaCy's data structures, and how to effectively combine statistical and rule-based approaches for text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures (1): Vocab, Lexemes and StringStore\n",
    "\n",
    "Now that you've had some real experience using spaCy's objects, it's time for you to learn more about what's actually going on under spaCy's hood.\n",
    "\n",
    "In this lesson, we'll take a look at the shared vocabulary and how spaCy deals with strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Vocab and string store\n",
    "\n",
    "SpaCy stores all shared data in a vocabulary, the Vocab.\n",
    "\n",
    "This includes words, but also the labels schemes for tags and entities.\n",
    "\n",
    "To save memory, all strings are encoded to hash IDs. If a word occurs more than once, we don't need to save it every time.\n",
    "\n",
    "Instead, spaCy uses a hash function to generate an ID and stores the string only once in the string store. The string store is available as nlp dot vocab dot strings.\n",
    "\n",
    "It's a lookup table that works in both directions. You can look up a string and get its hash, and look up a hash to get its string value. Internally, spaCy only communicates in hash IDs.\n",
    "\n",
    "Hash IDs can't be reversed, though. If a word in not in the vocabulary, there's no way to get its string. That's why we always need to pass around the shared vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "976"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "len(nlp.vocab.strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3197928453018144401\n"
     ]
    }
   ],
   "source": [
    "coffe_hash= nlp.vocab.strings['coffee']\n",
    "print(coffe_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"[E018] Can't retrieve string for hash '3197928453018144401'.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-d24d695fda14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcoffee_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcoffe_hash\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoffee_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mstrings.pyx\u001b[0m in \u001b[0;36mspacy.strings.StringStore.__getitem__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"[E018] Can't retrieve string for hash '3197928453018144401'.\""
     ]
    }
   ],
   "source": [
    "coffee_string = nlp.vocab.strings[coffe_hash]\n",
    "print(coffee_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "len(nlp.vocab.strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "len(nlp.vocab.strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffe_hash= nlp.vocab.strings['cafe']\n",
    "print(coffe_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffe_hash= nlp.vocab.strings[u'café']\n",
    "print(coffe_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.strings[99999999999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,len(nlp.vocab.strings)):\n",
    "    print(nlp.vocab.strings[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the hash for a string, we can look it up in nlp dot vocab dot strings.\n",
    "\n",
    "To get the string representation of a hash, we can look up the hash.\n",
    "\n",
    "A Doc object also exposes its vocab and strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "print('hash value:', nlp.vocab.strings['coffee'])\n",
    "print('string value:', nlp.vocab.strings[3197928453018144401])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "print('hash value:', doc.vocab.strings['coffee'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexemes: entries in the vocabulary\n",
    "\n",
    "Lexemes are context-independent entries in the vocabulary.\n",
    "\n",
    "You can get a lexeme by looking up a string or a hash ID in the vocab.\n",
    "\n",
    "Lexemes expose attributes, just like tokens.\n",
    "\n",
    "They hold context-independent information about a word, like the text, or whether the the word consists of alphanumeric characters.\n",
    "\n",
    "Lexemes don't have part-of-speech tags, dependencies or entity labels. Those depend on the context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "lexeme = nlp.vocab['coffee']\n",
    "\n",
    "# Print the lexical attributes\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contains the context-independent information about a word.\n",
    "\n",
    "Word text: lexeme.text and lexeme.orth (the hash).\n",
    "    \n",
    "Lexical attributes like lexeme.is_alpha.\n",
    "\n",
    "Not context-dependent part-of-speech tags, dependencies or entity labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/data-struct.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above one example:\n",
    "\n",
    "The Doc contains words in context – in this case, the tokens \"I\", \"love\" and \"coffee\" with their part-of-speech tags and dependencies.\n",
    "\n",
    "Each token refers to a lexeme, which knows the word's hash ID. To get the string representation of the word, spaCy looks up the hash in the string store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String to hashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "Look up the string “cat” in nlp.vocab.strings to get the hash.\n",
    "\n",
    "Look up the hash to get back the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I have a cat\")\n",
    "\n",
    "# Look up the hash for the word \"cat\"\n",
    "cat_hash = nlp.vocab.strings[\"cat\"]\n",
    "print(cat_hash)\n",
    "\n",
    "# Look up the cat_hash to get the string\n",
    "cat_string = nlp.vocab.strings[cat_hash]\n",
    "print(cat_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 \n",
    "\n",
    "Look up the string label “PERSON” in nlp.vocab.strings to get the hash.\n",
    "\n",
    "Look up the hash to get back the string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"David Bowie is a PERSON\")\n",
    "\n",
    "# Look up the hash for the string label \"PERSON\"\n",
    "person_hash = nlp.vocab.strings[\"PERSON\"]\n",
    "print(person_hash)\n",
    "\n",
    "# Look up the person_hash to get the string\n",
    "person_string = nlp.vocab.strings[person_hash]\n",
    "print(person_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"David Bowie is a PERSON\")\n",
    "\n",
    "# Look up the hash for the string label \"PERSON\"\n",
    "person_hash = nlp.vocab.strings[\"person\"]\n",
    "print(person_hash)\n",
    "\n",
    "# Look up the person_hash to get the string\n",
    "person_string = nlp.vocab.strings[person_hash]\n",
    "print(person_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"David Bowie is a PERSON\")\n",
    "\n",
    "# Look up the hash for the string label \"PERSON\"\n",
    "person_hash = nlp.vocab.strings[\"Person\"]\n",
    "print(person_hash)\n",
    "\n",
    "# Look up the person_hash to get the string\n",
    "person_string = nlp.vocab.strings[person_hash]\n",
    "print(person_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"David Bowie is a PERSON\")\n",
    "\n",
    "# Look up the hash for the string label \"PERSON\"\n",
    "person_hash = nlp.vocab.strings[\"pErson\"]\n",
    "print(person_hash)\n",
    "\n",
    "# Look up the person_hash to get the string\n",
    "person_string = nlp.vocab.strings[person_hash]\n",
    "print(person_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab, hashes and lexemes\n",
    "\n",
    "Why does this code throw an error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.lang.de import German\n",
    "\n",
    "# Create an English and German nlp object\n",
    "nlp = English()\n",
    "nlp_de = German()\n",
    "\n",
    "# Get the ID for the string 'Bowie'\n",
    "bowie_id = nlp.vocab.strings['Bowie']\n",
    "print(bowie_id)\n",
    "\n",
    "# Look up the ID for 'Bowie' in the vocab\n",
    "print(nlp_de.vocab.strings[bowie_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string 'Bowie' isn't in the German vocab, so the hash can't be resolved in the string store.\n",
    "\n",
    "Hashes can’t be reversed. To prevent this problem, add the word to the new vocab by processing a text or looking up the string, or use the same vocab to resolve the hash back to a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures(2)\n",
    "\n",
    "Now that you know all about the vocabulary and string store, we can take a look at the most important data structure: the Doc, and its views Token and Span."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Doc object\n",
    "\n",
    "The Doc is one of the central data structures in spaCy. It's created automatically when you process a text with the nlp object. But you can also instantiate the class manually.\n",
    "\n",
    "After creating the nlp object, we can import the Doc class from spacy dot tokens.\n",
    "\n",
    "Here we're creating a Doc from three words. The spaces are a list of boolean values indicating whether the word is followed by a space. Every token includes that information – even the last one!\n",
    "\n",
    "The Doc class takes three arguments: the shared vocab, the words and the spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an nlp object\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/span2.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Span is a slice of a Doc consisting of one or more tokens. The Span takes at least three arguments: the doc it refers to, and the start and end index of the span. Remember that the end index is exclusive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a Span manually, we can also import the class from spacy dot tokens. We can then instantiate it with the doc and the span's start and end index.\n",
    "\n",
    "To add an entity label to the span, we first need to look up the string in the string store. We can then provide it to the span as the label argument.\n",
    "\n",
    "The doc dot ents are writable, so we can add entities manually by overwriting it with a list of spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "# Create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "# Add span to the doc.ents\n",
    "doc.ents = [span_with_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few tips and tricks before we get started:\n",
    "\n",
    "The Doc and Span are very powerful and optimized for performance. They give you access to all references and relationships of the words and sentences.\n",
    "\n",
    "If your application needs to output strings, make sure to convert the doc as late as possible. If you do it too early, you'll lose all relationships between the tokens.\n",
    "\n",
    "To keep things consistent, try to use built-in token attributes wherever possible. For example, token dot i for the token index.\n",
    "\n",
    "Also, don't forget to always pass in the shared vocab!\n",
    "\n",
    "<b> Best practices </b>\n",
    "\n",
    "Doc and Span are very powerful and hold references and relationships of words and sentences\n",
    "\n",
    "  Convert result to strings as late as possible\n",
    "    \n",
    "  Use token attributes if available – for example, token.i for the token index\n",
    "\n",
    "Don't forget to pass in the shared vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Doc\n",
    "\n",
    "Let’s create some Doc objects from scratch!\n",
    "\n",
    "### Part 1\n",
    "\n",
    "Import the Doc from spacy.tokens.\n",
    "\n",
    "Create a Doc from the words and spaces. Don’t forget to pass in the vocab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"spaCy is cool!\"\n",
    "words = [\"spaCy\", \"is\", \"cool\", \"!\"]\n",
    "spaces = [True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Import the Doc from spacy.tokens.\n",
    "\n",
    "Create a Doc from the words and spaces. Don’t forget to pass in the vocab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Go, get started!\"\n",
    "words = [\"Go\", \",\", \"get\", \"started\", \"!\"]\n",
    "spaces = [False, True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> DOC Docstring:     </b>\n",
    "    \n",
    "A sequence of Token objects. Access sentences and named entities, export\n",
    "annotations to numpy arrays, losslessly serialize to compressed binary\n",
    "strings. The `Doc` object holds an array of `TokenC` structs. The\n",
    "Python-level `Token` and `Span` objects are views of this array, i.e.\n",
    "they don't own the data themselves.\n",
    "\n",
    "EXAMPLE: Construction 1\n",
    "    >>> doc = nlp(u'Some text')\n",
    "\n",
    "    Construction 2\n",
    "    >>> from spacy.tokens import Doc\n",
    "    >>> doc = Doc(nlp.vocab, words=[u'hello', u'world', u'!'],\n",
    "                  spaces=[True, False, False])\n",
    "Init docstring:\n",
    "Create a Doc object.\n",
    "\n",
    "vocab (Vocab): A vocabulary object, which must match any models you\n",
    "    want to use (e.g. tokenizer, parser, entity recognizer).\n",
    "    \n",
    "words (list or None): A list of unicode strings to add to the document\n",
    "    as words. If `None`, defaults to empty list.\n",
    "    \n",
    "spaces (list or None): A list of boolean values, of the same length as\n",
    "    words. True means that the word is followed by a space, False means\n",
    "    it is not. If `None`, defaults to `[True]*len(words)`\n",
    "    \n",
    "user_data (dict or None): Optional extra data to attach to the Doc.\n",
    "\n",
    "RETURNS (Doc): The newly constructed object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "Import the Doc frmmom spacy.tokens.\n",
    "\n",
    "Complete the words and spaces to match the desired text and create a doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Oh, really?!\"\n",
    "words = [\"Oh\", \",\", \"really\", \"?\",\"!\"]\n",
    "spaces = [False, True, False, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs, spans and entities from scratch\n",
    "\n",
    "In this exercise, you’ll create the Doc and Span objects manually, and update the named entities – just like spaCy does behind the scenes. \n",
    "\n",
    "A shared nlp object has already been created.\n",
    "\n",
    "- Import the Doc and Span classes from spacy.tokens.\n",
    "\n",
    "- Use the Doc class directly to create a doc from the words and spaces.\n",
    "\n",
    "- Create a Span for “David Bowie” from the doc and assign it the label \"PERSON\".\n",
    "\n",
    "- Overwrite the doc.ents with a list of one entity, the “David Bowie” span.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label=\"PERSON\")\n",
    "print(span.text, span.label_)\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "doc.ents = [span]\n",
    "\n",
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label=\"PERSON\")\n",
    "print(span.text, span.label_)\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "doc.ents = [span]\n",
    "\n",
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structures and best practices\n",
    "\n",
    "The code in this example is trying to analyze a text and collect all proper nouns that are followed by a verb.\n",
    "\n",
    "### Part 1\n",
    "\n",
    "Why is the code bad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin is a nice city\")\n",
    "\n",
    "# Get all tokens and part-of-speech tags\n",
    "token_texts = [token.text for token in doc]\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # Check if the current token is a proper noun\n",
    "    if pos == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if pos_tags[index + 1] == \"VERB\":\n",
    "            result = token_texts[index]\n",
    "            print(\"Found proper noun before a verb:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token_texts)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spacy.explain('PROPN'))\n",
    "print(spacy.explain('VERB'))\n",
    "print(spacy.explain('DET'))\n",
    "print(spacy.explain('ADJ'))\n",
    "print(spacy.explain('NOUN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "\n",
    "It only uses lists of strings instead of native token attributes. This is often less efficient, and can't express complex relationships.\n",
    "\n",
    "Always convert the results to strings as late as possible, and try to use native token attributes to keep things consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Part 2\n",
    "Rewrite the code to use the native token attributes instead of lists of token_texts and pos_tags.\n",
    "Loop over each token in the doc and check the token.pos_ attribute.\n",
    "Use doc[token.i + 1] to check for the next token and its .pos_ attribute.\n",
    "If a proper noun before a verb is found, print its token.text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "\n",
    "- Rewrite the code to use the native token attributes instead of lists of token_texts and pos_tags.\n",
    "- Loop over each token in the doc and check the token.pos_ attribute.\n",
    "- Use doc[token.i + 1] to check for the next token and its .pos_ attribute.\n",
    "- If a proper noun before a verb is found, print its token.text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin is a nice city\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Check if the current token is a proper noun\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if doc[token.i + 1].pos_ == \"VERB\":\n",
    "            print(\"Found proper noun before a verb:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors and semantic similarities\n",
    "\n",
    "In this lesson, you'll learn how to use spaCy to predict how similar documents, spans or tokens are to each other.\n",
    "\n",
    "You'll also learn about how to use word vectors and how to take advantage of them in your NLP application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy can compare two objects and predict how similar they are – for example, documents, spans or single tokens.\n",
    "\n",
    "The Doc, Token and Span objects have a dot similarity method that takes another object and returns a floating point number between 0 and 1, indicating how similar they are.\n",
    "\n",
    "One thing that's very important: In order to use similarity, you need a larger spaCy model that has word vectors included.\n",
    "\n",
    "<b>For example, the medium or large English model – but not the small one. So if you want to use vectors, always go with a model that ends in \"md\" or \"lg\". You can find more details on this in the models documentation.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spaCy can compare two objects and predict similarity\n",
    "- Doc.similarity(), Span.similarity() and Token.similarity()\n",
    "- Take another object and return a similarity score (0 to 1)\n",
    "- Important: needs a model that has word vectors included, for example:\n",
    "    - ✅ en_core_web_md (medium model)\n",
    "    - ✅ en_core_web_lg (large model)\n",
    "    - 🚫 NOT en_core_web_sm (small model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example. Let's say we want to find out whether two documents are similar.\n",
    "\n",
    "First, we load the medium English model, \"en_core_web_md\".\n",
    "\n",
    "We can then create two doc objects and use the first doc's similarity method to compare it to the second.\n",
    "\n",
    "Here, a fairly high similarity score of 0.86 is predicted for \"I like fast food\" and \"I like pizza\".\n",
    "\n",
    "The same works for tokens.\n",
    "\n",
    "According to the word vectors, the tokens \"pizza\" and \"pasta\" are kind of similar, and receive a score of 0.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load a larger model with vectors\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two tokens\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the similarity methods to compare different types of objects.\n",
    "\n",
    "For example, a document and a token.\n",
    "\n",
    "Here, the similarity score is pretty low and the two objects are considered fairly dissimilar.\n",
    "\n",
    "Here's another example comparing a span – \"pizza and pasta\" – to a document about McDonalds.\n",
    "\n",
    "The score returned here is 0.61, so it's determined to be kind of similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare a document with a token\n",
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare a span with a document\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how does spaCy do this under the hood?\n",
    "\n",
    "Similarity is determined using word vectors, multi-dimensional representations of meanings of words.\n",
    "\n",
    "You might have heard of Word2Vec, which is an algorithm that's often used to train word vectors from raw text.\n",
    "\n",
    "Vectors can be added to spaCy's statistical models.\n",
    "\n",
    "By default, the similarity returned by spaCy is the cosine similarity between two vectors – but this can be adjusted if necessary.\n",
    "\n",
    "Vectors for objects consisting of several tokens, like the Doc and Span, default to the average of their token vectors.\n",
    "\n",
    "That's also why you usually get more value out of shorter phrases with fewer irrelevant words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does spaCy predict similarity?\n",
    "- Similarity is determined using word vectors\n",
    "- Multi-dimensional meaning representations of words\n",
    "- Generated using an algorithm like Word2Vec and lots of text\n",
    "- Can be added to spaCy's statistical models\n",
    "- Default: cosine similarity, but can be adjusted\n",
    "- Doc and Span vectors default to average of token vectors\n",
    "- Short phrases are better than long documents with many irrelevant words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give you an idea of what those vectors look like, here's an example.\n",
    "\n",
    "First, we load the medium model again, which ships with word vectors.\n",
    "\n",
    "Next, we can process a text and look up a token's vector using the dot vector attribute.\n",
    "\n",
    "The result is a 300-dimensional vector of the word \"banana\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a larger model with vectors\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "doc = nlp(\"I have a banana\")\n",
    "# Access the vector via the token.vector attribute\n",
    "print(doc[3].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting similarity can be useful for many types of applications. For example, to recommend a user similar texts based on the ones they have read. It can also be helpful to flag duplicate content, like posts on an online platform.\n",
    "\n",
    "However, it's important to keep in mind that there's no objective definition of what's similar and what isn't. It always depends on the context and what your application needs to do.\n",
    "\n",
    "Here's an example: spaCy's default word vectors assign a very high similarity score to \"I like cats\" and \"I hate cats\". This makes sense, because both texts express sentiment about cats. But in a different application context, you might want to consider the phrases as very dissimilar, because they talk about opposite sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I hate cats\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Useful for many applications: recommendation systems, flagging duplicates etc.\n",
    "- There's no objective definition of \"similarity\"\n",
    "- Depends on the context and what application needs to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting word vectors\n",
    "\n",
    "In this exercise, you’ll use a larger English model, which includes around 20.000 word vectors. Because vectors take a little longer to load, we’re using a slightly compressed version of it than the one you can download with spaCy. The model is already pre-installed.\n",
    "\n",
    "- Load the medium 'en_core_web_md' model with word vectors.\n",
    "- Print the vector for \"bananas\" using the token.vector attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_md model\n",
    "# nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Two bananas in pyjamas\")\n",
    "\n",
    "# Get the vector for the token \"bananas\"\n",
    "bananas_vector = doc[1].vector\n",
    "print(bananas_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing similarities\n",
    "\n",
    "In this exercise, you’ll be using spaCy’s similarity methods to compare Doc, Token and Span objects and get similarity scores.\n",
    "\n",
    "Part 1\n",
    "\n",
    "Use the doc.similarity method to compare doc1 to doc2 and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc1 = nlp(\"It's a warm summer day\")\n",
    "doc2 = nlp(\"It's sunny outside\")\n",
    "\n",
    "# Get the similarity of doc1 and doc2\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2\n",
    "\n",
    "Use the token.similarity method to compare token1 to token2 and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"TV and books\")\n",
    "token1, token2 = doc[0], doc[2]\n",
    "\n",
    "# Get the similarity of the tokens \"TV\" and \"books\"\n",
    "similarity = token1.similarity(token2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3\n",
    "\n",
    "Create spans for “great restaurant”/“really nice bar”.\n",
    "\n",
    "Use span.similarity to compare them and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "\n",
    "# Create spans for \"great restaurant\" and \"really nice bar\"\n",
    "span1 = doc[3:5]\n",
    "span2 = doc[12:15]\n",
    "\n",
    "# Get the similarity of the spans\n",
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I like a hot cup of coffe but I hate a cold bread with butter.\")\n",
    "\n",
    "span1 = doc[0:7]\n",
    "span2 = doc[8:15]\n",
    "\n",
    "# Get the similarity of the spans\n",
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining models and rules\n",
    "\n",
    "Combining statistical models with rule-based systems is one of the most powerful tricks you should have in your NLP toolbox.\n",
    "\n",
    "In this lesson, we'll take a look at how to do it with spaCy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical predictions vs. rules \n",
    "\n",
    "Statistical models are useful if your application needs to be able to generalize based on a few examples.\n",
    "\n",
    "For instance, detecting product or person names usually benefits from a statistical model. Instead of providing a list of all person names ever, your application will be able to predict whether a span of tokens is a person name. Similarly, you can predict dependency labels to find subject/object relationships.\n",
    "\n",
    "To do this, you would use spaCy's entity recognizer, dependency parser or part-of-speech tagger.\n",
    "\n",
    "Rule-based approaches on the other hand come in handy if there's a more or less finite number of instances you want to find. For example, all countries or cities of the world, drug names or even dog breeds.\n",
    "\n",
    "In spaCy, you can achieve this with custom tokenization rules, as well as the matcher and phrase matcher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/vs.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECAP: Rule-based matching\n",
    "\n",
    "In the last chapter, you learned how to use spaCy's rule-based matcher to find complex patterns in your texts. Here's a quick recap.\n",
    "\n",
    "The matcher is initialized with the shared vocabulary – usually nlp dot vocab.\n",
    "\n",
    "Patterns are lists of dictionaries, and each dictionary describes one token and its attributes. Patterns can be added to the matcher using the matcher dot add method.\n",
    "\n",
    "Operators let you specify how often to match a token. For example, \"+\" will match one or more times.\n",
    "\n",
    "Calling the matcher on a doc object will return a list of the matches. Each match is a tuple consisting of an ID, and the start and end token index in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with the shared vocab\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Patterns are lists of dictionaries describing the tokens\n",
    "pattern = [{'LEMMA': 'love', 'POS': 'VERB'}, {'LOWER': 'cats'}]\n",
    "matcher.add('LOVE_CATS', None, pattern)\n",
    "\n",
    "# Operators can specify how often a token should be matched\n",
    "pattern = [{'TEXT': 'very', 'OP': '+'}, {'TEXT': 'happy'}]\n",
    "\n",
    "# Calling matcher on doc returns list of (match_id, start, end) tuples\n",
    "doc = nlp(\"I love cats and I'm very very happy\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RETURNS (list): A list of `(key, start, end)` tuples,\n",
    "    describing the matches. A match tuple describes a span\n",
    "    `doc[start:end]`. The `label_id` and `key` are both integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding statistical predictions \n",
    "\n",
    "Here's an example of a matcher rule for \"golden retriever\".\n",
    "\n",
    "If we iterate over the matches returned by the matcher, we can get the match ID and the start and end index of the matched span. We can then find out more about it. Span objects give us access to the original document and all other token attributes and linguistic features predicted by the model.\n",
    "\n",
    "For example, we can get the span's root token. If the span consists of more than one token, this will be the token that decides the category of the phrase. For example, the root of \"Golden Retriever\" is \"Retriever\". We can also find the head token of the root. This is the syntactic \"parent\" that governs the phrase – in this case, the verb \"have\".\n",
    "\n",
    "Finally, we can look at the previous token and its attributes. In this case, it's a determiner, the article \"a\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('DOG', None, [{'LOWER': 'golden'}, {'LOWER': 'retriever'}])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)\n",
    "    # Get the span's root token and root head token\n",
    "    print('Root token:', span.root.text)\n",
    "    print('Root head token:', span.root.head.text)\n",
    "    # Get the previous token and its POS tag\n",
    "    print('Previous token:', doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient phrase matching (1) \n",
    "\n",
    "The phrase matcher is another helpful tool to find sequences of words in your data.\n",
    "\n",
    "It performs a keyword search on the document, but instead of only finding strings, it gives you direct access to the tokens in context.\n",
    "\n",
    "It takes Doc objects as patterns.\n",
    "\n",
    "It's also really fast.\n",
    "\n",
    "This makes it very useful for matching large dictionaries and word lists on large volumes of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PhraseMatcher like regular expressions or keyword search – but with access to the tokens!\n",
    "- Takes Doc object as patterns\n",
    "- More efficient and faster than the Matcher\n",
    "- Great for matching large word lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example.\n",
    "\n",
    "The phrase matcher can be imported from spacy dot matcher and follows the same API as the regular matcher.\n",
    "\n",
    "Instead of a list of dictionaries, we pass in a Doc object as the pattern.\n",
    "\n",
    "We can then iterate over the matches in the text, which gives us the match ID, and the start and end of the match. This lets us create a Span object for the matched tokens \"Golden Retriever\" to analyze it in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add('DOG', None, pattern)\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Get the matched span\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging patterns (1)\n",
    "\n",
    "Why does this pattern not match the tokens “Silicon Valley” in the doc?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "matcher.add('Sil', None, [{'LOWER': 'silicon'}, {'TEXT': ' '}, {'LOWER': 'valley'}])\n",
    "\n",
    "doc = nlp(\"Can Silicon Valley workers rein in big tech from within?\")\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Get the matched span\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer doesn't create tokens for single spaces, so there's no token with the value ' ' in between. The tokenizer already takes care of splitting off whitespace and each dictionary in the pattern describes one token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debuggind patterns (2)\n",
    "\n",
    "Both patterns in this exercise contain mistakes and won’t match as expected. Can you fix them? If you get stuck, try printing the tokens in the doc to see how the text will be split and adjust the pattern so that each dictionary represents one token.\n",
    "\n",
    "- Edit pattern1 so that it correctly matches all case-insensitive mentions of \"Amazon\" plus a title-cased proper noun.\n",
    "- Edit pattern2 so that it correctly matches all case-insensitive mentions of \"ad-free\", plus the following noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{\"LOWER\": \"Amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "pattern2 = [{\"LOWER\": \"ad-free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", None, pattern1)\n",
    "matcher.add(\"PATTERN2\", None, pattern2)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{\"LOWER\": \"amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "pattern2 = [{\"LOWER\": \"ad\"}, {\"TEXT\": \"-\"}, {\"LOWER\": \"free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", None, pattern1)\n",
    "matcher.add(\"PATTERN2\", None, pattern2)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient phrase matching ------\n",
    "\n",
    "Sometimes it’s more efficient to match exact strings instead of writing patterns describing the individual tokens. This is especially true for finite categories of things – like all countries of the world. We already have a list of countries, so let’s use this as the basis of our information extraction script. A list of string names is available as the variable COUNTRIES.\n",
    "\n",
    "- Import the PhraseMatcher and initialize it with the shared vocab as the variable matcher.\n",
    "- Add the phrase patterns and call the matcher on the doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from spacy.lang.en import English\n",
    "\n",
    "with open(\"exercises/countries.json\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting countries and relationships ----\n",
    "\n",
    "In the previous exercise, you wrote a script using spaCy’s PhraseMatcher to find country names in text. Let’s use that country matcher on a longer text, analyze the syntax and update the document’s entities with the matched countries.\n",
    "\n",
    "- Iterate over the matches and create a Span with the label \"GPE\" (geopolitical entity).\n",
    "- Overwrite the entities in doc.ents and add the matched span.\n",
    "- Get the matched span’s root head token.\n",
    "- Print the text of the head token and the span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "import json\n",
    "\n",
    "with open(\"exercises/countries.json\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "with open(\"exercises/country_text.txt\") as f:\n",
    "    TEXT = f.read()\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# Create a doc and find matches in it\n",
    "doc = nlp(TEXT)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\"\n",
    "    span = Span(doc, start, end, label=\"GPE\")\n",
    "\n",
    "    # Overwrite the doc.ents and add the span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "    # Get the span's root head token\n",
    "    span_root_head = span.root.head\n",
    "    # Print the text of the span root's head token and the span text\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "\n",
    "# Print the entities in the document\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"GPE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contens",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "332.818px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
